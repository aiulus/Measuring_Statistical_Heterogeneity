{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fe5ec5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m kstest, anderson_ksamp, cumfreq, ks_2samp, cramervonmises, chisquare, entropy, wasserstein_distance\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_data, getDivs, map_to_prob, vis_divergence, tensor_to_csv\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mempirical_distribution\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ECDF\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartitioning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partition\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import kstest, anderson_ksamp, cumfreq, ks_2samp, cramervonmises, chisquare, entropy, wasserstein_distance\n",
    "from src.utils.utils import get_data, getDivs, map_to_prob, vis_divergence, tensor_to_csv\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from src.utils.partitioning import partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48ee38f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(y_train, subset_map, mode):\n",
    "    def kl_divergence(p, q):\n",
    "        return sum(p[i] * math.log2(p[i] / q[i]) for i in range(len(p)) if q[i] != 0 and p[i] != 0)\n",
    "\n",
    "    # Jensen-Shannon Divergence\n",
    "    def js_divergence(p, q):\n",
    "        m = 0.5 * (p + q)\n",
    "        return 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)\n",
    "\n",
    "    # Gini Coefficient\n",
    "    def gini(x):\n",
    "        total = 0\n",
    "        for i, xi in enumerate(x[:-1], 1):\n",
    "            total += np.sum(np.abs(xi - x[i:]))\n",
    "        return total / (len(x) ** 2 * np.mean(x))\n",
    "\n",
    "    N = y_train.shape[0]\n",
    "    probs_original, probs = map_to_prob(y_train, {0: np.arange(N)})[0], map_to_prob(y_train, subset_map)\n",
    "    stats, pvals = [], []\n",
    "\n",
    "    mode_dict = {\n",
    "        \"kolmogorov-smirnov\": lambda x: kstest(x, probs_original),\n",
    "        \"empirical\": lambda x: ks_2samp(ECDF(x)(x), ECDF(probs_original)(probs_original)),\n",
    "        \"cramer-von-mises\": lambda x: cramervonmises(ECDF(x), ECDF(probs_original)),\n",
    "        \"pearson-chi-squared\": lambda x: chisquare(x * N, probs_original * N),\n",
    "        \"anderson-darling\": anderson_ksamp\n",
    "    }\n",
    "\n",
    "    mode_entropy = {\n",
    "        \"kl\": lambda x: kl_divergence(x, probs_original),\n",
    "        \"js\": lambda x: js_divergence(x, probs_original),\n",
    "        \"wd\": lambda x: wasserstein_distance(x, probs_original),\n",
    "        \"gini\": gini\n",
    "    }\n",
    "\n",
    "    if (mode in mode_entropy):\n",
    "        try:\n",
    "            test_func = mode_entropy[mode]\n",
    "        except KeyError:\n",
    "            raise ValueError(f\"Invalid mode: {mode}\")\n",
    "\n",
    "        stats, pvals = [test_func(probs[j]) for j in range(len(probs))], []\n",
    "    else:\n",
    "        try:\n",
    "            test_func = mode_dict[mode]\n",
    "        except KeyError:\n",
    "            raise ValueError(f\"Invalid mode: {mode}\")\n",
    "\n",
    "        stats, pvals = zip(*[test_func(probs[j]) for j in range(len(probs))])\n",
    "\n",
    "    return stats, pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf56f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(dataset_type, alpha_vector, path, mode_part, mode_test, mode_plot, n_clients, plot=False,\n",
    "                   logpath=None, save=False):\n",
    "    train, test = get_data(dataset_type, path)\n",
    "    x_train, y_train, x_test, y_test = train[0], train[1], test[0], test[1]\n",
    "    evol_stat, evol_pval = [], []\n",
    "\n",
    "    for j in range(len(alpha_vector)):\n",
    "        a_j = alpha_vector[j]\n",
    "        subset_map = partition(train, mode_part, n_clients, a_j)\n",
    "        if save:\n",
    "            tensor_to_csv(x_train, y_train, subset_map, j)\n",
    "        stats, pvals = distance(y_train, subset_map, mode_test)\n",
    "        mean_teststat_j = np.mean(stats)\n",
    "        mean_pval_j = np.mean(pvals)\n",
    "        evol_stat.append(mean_teststat_j)\n",
    "        evol_pval.append(mean_pval_j)\n",
    "\n",
    "    title_formats_part = {\n",
    "        \"hetero-dir\": \"Mean divergence from original distribution under heterogeneous partitioning via Dirichlet distribution\",\n",
    "        \"hetero-gaussian\": \"Mean divergence from original distribution under heterogeneous partitioning via Gaussian distribution\",\n",
    "        \"homo\": \"A homogeneous partitioning with {n_clients} subsets\",\n",
    "        \"quant\": \"A quantity-based heterogeneous partitioning with {n_clients} subsets\"\n",
    "    }\n",
    "\n",
    "    title_formats_test = {\n",
    "        \"kolmogorov-smirnov\": \"Test Statistic: Kolmogorov-Smirnov\",\n",
    "        \"empirical\": \"Test Statistic: Empirical Distribution\",\n",
    "        \"kl\": \"Test Statistic: Kullback-Leibler Divergence (Entropy-Based)\",\n",
    "        \"js\": \"Test Statistic: Jensen-Shannon Divergence (Entropy-Based)\",\n",
    "        \"wd\": \"Test Statistic: Wasserstein Distance\",\n",
    "        \"gini\": \"Test Statistic: Gini Coefficient\"\n",
    "    }\n",
    "\n",
    "    main_title = title_formats_part.get(mode_part, \"\")\n",
    "    main_title = main_title.format(n_clients=n_clients)\n",
    "    appendage = title_formats_test.get(mode_test, \"\")\n",
    "    main_title = dataset_type + ': ' + main_title + '\\n' + appendage\n",
    "\n",
    "    if mode_part == \"quant\":\n",
    "        l, u = tuple(round(x, 1) for x in (min(alpha_vector), max(alpha_vector)))\n",
    "        xlab = f'[{u}:{l}], set of divisors for {u}'\n",
    "    elif len(alpha_vector) <= 15:\n",
    "        xlab = [f'Î±_{j}={alpha}' for j, alpha in enumerate(alpha_vector)]\n",
    "    else:\n",
    "        l, u, steps = tuple(round(x, 1) for x in (\n",
    "        min(alpha_vector), max(alpha_vector), (max(alpha_vector) - min(alpha_vector)) / (len(alpha_vector) - 1)))\n",
    "        xlab = f'[{l}:{u}], step size = {steps}'\n",
    "\n",
    "    if plot:\n",
    "        vis_divergence(evol_stat, mode_plot, main_title, xlab)\n",
    "\n",
    "    return (evol_stat, evol_pval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "846ac8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_plot(dataset_type, alpha_vector, path, mode_part, mode_test, mode_plot, n_clients, runs):\n",
    "    vals = []\n",
    "    for j in range(runs):\n",
    "        stats = np.array(\n",
    "            run_experiment(dataset_type, alpha_vector, path, mode_part, mode_test, mode_plot, n_clients, plot=False)[0])\n",
    "        vals.append(stats)\n",
    "\n",
    "    p = pd.DataFrame(vals)\n",
    "    means = []\n",
    "    stds = []\n",
    "    for j in range(p.shape[1]):\n",
    "        col = np.array(p[j])\n",
    "        means.append(col.mean())\n",
    "        # sigma_j = math.sqrt(sum([math.pow(x - np.array(p[j]).mean(), 2) for x in p[j]])/len(p[j]))\n",
    "        stds.append(np.array(p[j]).std())\n",
    "    means, stds = np.array(means), np.array(stds)\n",
    "\n",
    "    part_dict = {\n",
    "        \"hetero-dir\": \"Dirichlet-Partitioning\",\n",
    "        \"hetero-gaussian\": \"Gaussian Partitioning\",\n",
    "        \"homo\": \"Homogeneous Partitioning\",\n",
    "        \"quant\": \"Quantity-based Heterogeneous Partitioning\"\n",
    "    }\n",
    "\n",
    "    test_dict = {\n",
    "        \"kolmogorov-smirnov\": \"Test Statistic: Kolmogorov-Smirnov\",\n",
    "        \"empirical\": \"Test Statistic: Empirical Distribution\",\n",
    "        \"kl\": \"Test Statistic: Kullback-Leibler Divergence (Entropy-Based)\",\n",
    "        \"js\": \"Test Statistic: Jensen-Shannon Divergence (Entropy-Based)\",\n",
    "        \"wd\": \"Test Statistic: Wasserstein Distance\",\n",
    "        \"gini\": \"Test Statistic: Gini Coefficient\"\n",
    "    }\n",
    "\n",
    "    main_title = f'Evolution of divergence over {runs} epochs.\\n ' + part_dict.get(mode_part,\n",
    "                                                                                   \"\") + \", \" + test_dict.get(mode_test,\n",
    "                                                                                                              \"\")\n",
    "\n",
    "    plt.figure(figsize=(5, 3), dpi=300)\n",
    "    plt.plot(np.arange(len(means)), means)\n",
    "    plt.fill_between(np.arange(len(means)), means - stds, means + stds, color='gray', alpha=0.2)\n",
    "    plt.grid(linestyle='-', linewidth=0.5)\n",
    "    plt.suptitle(main_title, fontsize='small')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a222b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
