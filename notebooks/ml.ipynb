{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e980cf73",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseCategoricalCrossentropy, CategoricalCrossentropy\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# from ann_visualizer.visualize import ann_viz\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getDivs, tensor_to_csv, plot_curve, normalized_euclidian_transform\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartitioning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partition\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import optimizers\n",
    "from keras.datasets import mnist, cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "# from ann_visualizer.visualize import ann_viz\n",
    "\n",
    "from src.utils.utils import getDivs, tensor_to_csv, plot_curve, normalized_euclidian_transform\n",
    "from src.utils.partitioning import partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9122045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_architecture(dataset_type, model_type):\n",
    "    cnn_layers = {\n",
    "        \"MNIST\": [Conv2D(filters=64,\n",
    "                         kernel_size=3,\n",
    "                         activation='relu',\n",
    "                         input_shape=(28, 28, 1)),\n",
    "                  MaxPooling2D(2),\n",
    "                  Conv2D(filters=64,\n",
    "                         kernel_size=3,\n",
    "                         activation='relu'),\n",
    "                  MaxPooling2D(2),\n",
    "                  Conv2D(filters=64,\n",
    "                         kernel_size=3,\n",
    "                         activation='relu'),\n",
    "                  Flatten(),\n",
    "                  Dropout(0.2),\n",
    "                  Dense(10, activation='softmax')\n",
    "                  ],\n",
    "        \"CIFAR10\": [Conv2D(32, (3, 3),\n",
    "                           activation='relu',\n",
    "                           input_shape=(32, 32, 3)),\n",
    "                    MaxPooling2D((2, 2)),\n",
    "                    Conv2D(64, (3, 3),\n",
    "                           activation='relu'),\n",
    "                    MaxPooling2D((2, 2)),\n",
    "                    Conv2D(64, (3, 3),\n",
    "                           activation='relu'),\n",
    "                    Flatten(),\n",
    "                    Dense(64, activation='relu'),\n",
    "                    Dense(10)]\n",
    "    }\n",
    "\n",
    "    cnn_loss = {\n",
    "        \"MNIST\": CategoricalCrossentropy(),\n",
    "        \"CIFAR10\": CategoricalCrossentropy()\n",
    "    }\n",
    "\n",
    "    # TODO: Find best configuration for CIFAR10\n",
    "    nn_layers = {\n",
    "        \"MNIST\": [\n",
    "            Flatten(input_shape=(28, 28)),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(rate=0.4),\n",
    "            Dense(units=10, activation='softmax')\n",
    "        ],\n",
    "        \"CIFAR10\": [\n",
    "            Flatten(input_shape=(32, 32)),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(rate=0.4),\n",
    "            Dense(units=10, activation='softmax')\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    nn_loss = {\n",
    "        \"MNIST\": CategoricalCrossentropy(),\n",
    "        \"CIFAR10\": CategoricalCrossentropy()\n",
    "    }\n",
    "\n",
    "    if model_type == \"nn\":\n",
    "        return nn_layers.get(dataset_type), nn_loss.get(dataset_type)\n",
    "    elif model_type == \"cnn\":\n",
    "        return cnn_layers.get(dataset_type), cnn_loss.get(dataset_type)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid mode: {model_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87884da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(dataset_type, model_type, data, n_epochs, lrate, batch_size=None):\n",
    "    train_data, train_targets, test_data, test_targets = data\n",
    "\n",
    "    layers, loss_function = get_architecture(dataset_type, model_type)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "\n",
    "    model.compile(loss=loss_function,\n",
    "                  optimizer=optimizers.Adam(learning_rate=lrate),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # TODO: batch_size --> SGD\n",
    "    # 60000/32 = 1875 datapoints per epoch\n",
    "    history = model.fit(train_data, train_targets,\n",
    "                        epochs=n_epochs,\n",
    "                        validation_data=(test_data, test_targets), batch_size=32)\n",
    "    # batch_size=4000\n",
    "    loss, accuracy = model.evaluate(test_data, test_targets, batch_size=batch_size)\n",
    "\n",
    "    return accuracy, loss, history.epoch, pd.DataFrame(history.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc8bd74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model_type, dataset_type, partitioning_type, n_clients, alpha, n_epochs, lrate=0.003,\n",
    "                   batch_size=None):\n",
    "    dataset_loaders = {\n",
    "        \"MNIST\": mnist.load_data,\n",
    "        \"CIFAR10\": normalized_euclidian_transform if model_type == \"nn\" else cifar10.load_data\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        loader = dataset_loaders[dataset_type]\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Invalid mode: {dataset_type}\")\n",
    "\n",
    "    train, test = loader()\n",
    "    test_data, test_targets = test[0], test[1]\n",
    "\n",
    "    test_targets = to_categorical(test_targets, num_classes=10)\n",
    "    if dataset_type == \"MNIST\":\n",
    "        test_data = test_data.reshape((-1,) + test_data.shape[1:3] + (1,))\n",
    "\n",
    "    subset_map = partition(dataset_type, partitioning_type, n_clients, alpha)\n",
    "    accuracies = []\n",
    "\n",
    "    for j in range(len(subset_map)):\n",
    "        subset_j_data = train[0][subset_map[j]]\n",
    "        subset_j_targets = train[1][subset_map[j]]\n",
    "\n",
    "        # convert to one-hot vector\n",
    "        # subset_j_targets = to_categorical(subset_j_targets)\n",
    "        subset_j_targets = to_categorical(subset_j_targets, num_classes=10)\n",
    "\n",
    "        if dataset_type == \"MNIST\":\n",
    "            subset_j_data = subset_j_data.reshape((-1,) + subset_j_data.shape[1:3] + (1,))\n",
    "\n",
    "        if model_type == \"nn\":\n",
    "            subset_j_data = subset_j_data.astype('float32') / 255\n",
    "            test_data = test_data.astype('float') / 255\n",
    "\n",
    "        data_j = (subset_j_data, subset_j_targets, test_data, test_targets)\n",
    "        # accuracy, loss, epochs, hist = build_model(dataset_type, model_type, data_j, n_epochs, lrate)\n",
    "        accuracy = build_model(dataset_type, model_type, data_j, n_epochs, lrate)[0]\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51040ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_experiment(model_type, dataset_type, partitioning_type, n_clients, alpha, epochs, lrate=0.003):\n",
    "    dict_accuracies = {}\n",
    "    accuracies_mean = []\n",
    "    accuracies_std = []\n",
    "\n",
    "    for j, alpha_j in enumerate(alpha):\n",
    "        accuracies_j = run_experiment(model_type, dataset_type, partitioning_type, n_clients, alpha_j, epochs, lrate)\n",
    "        dict_accuracies.update({j: accuracies_j})\n",
    "        mean_j = np.array(accuracies_j).mean()\n",
    "        accuracies_mean.append(mean_j)\n",
    "        std_j = np.array(accuracies_j).std()\n",
    "        accuracies_std.append(std_j)\n",
    "\n",
    "    plt.figure(figsize=(5, 3), dpi=300)\n",
    "    plt.plot(np.arange(len(accuracies_mean)), accuracies_mean)\n",
    "    plt.fill_between(np.arange(len(accuracies_mean)), np.array(accuracies_mean) - np.array(accuracies_std),\n",
    "                     np.array(accuracies_mean) + np.array(accuracies_std), color='gray', alpha=0.2)\n",
    "    plt.grid(linestyle='-', linewidth=0.5)\n",
    "    # plt.suptitle(main_title, fontsize='small')\n",
    "    plt.show()\n",
    "\n",
    "    return dict_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd5414f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
